---
layout: post
title:  "Benchmarking of Language Models"
date:   2024-06-13 14:07:47 +0300
categories: benchmarking llm RnD
---

<link rel="stylesheet" type="text/css" href="https://cdn.datatables.net/1.11.3/css/jquery.dataTables.css">
<script type="text/javascript" charset="utf8" src="https://code.jquery.com/jquery-3.5.1.js"></script>
<script type="text/javascript" charset="utf8" src="https://cdn.datatables.net/1.11.3/js/jquery.dataTables.js"></script>


<div class="content-with-image">
  <div class="text-content">
   <h2>Introduction</h2>
    This research aimed to benchmark the performance of various language models (LLMs) in the context of GreenM's operations. Specifically, we evaluated the effectiveness of these models in analyzing client feedback in healthcare services and responding to inquiries about corporate documents. The goal was to assess the adequacy, integrity, and usefulness of the models' responses, which would contribute to a more objective human evaluation of their performance. 
  </div>
  <div class="image-content">
    <svg width="311.5" height="311.5" viewBox="0 0 623 623" fill="none" xmlns="http://www.w3.org/2000/svg">
      <path fill-rule="evenodd" clip-rule="evenodd" d="M431.469 401.037L472.226 357.265L369.49 114.147L240.817 82.1474L140.964 384.214L267.04 460.6L431.469 401.037Z" fill="#020000"/>
      <path fill-rule="evenodd" clip-rule="evenodd" d="M263.456 211.199C251.551 211.507 241.634 202.042 241.336 190.096C241.044 178.249 250.48 168.284 262.39 167.982C274.197 167.678 284.115 177.139 284.412 188.992C284.705 200.937 275.269 210.897 263.462 211.2L263.456 211.199ZM263.022 196.119C259.536 196.207 256.662 193.463 256.572 189.97C256.487 186.57 259.224 183.684 262.71 183.591C266.098 183.503 268.972 186.247 269.057 189.647C269.141 193.144 266.405 196.03 263.016 196.118L263.022 196.119Z" fill="white"/>
      <path fill-rule="evenodd" clip-rule="evenodd" d="M349.42 206.455C345.723 212.616 337.701 214.694 331.478 210.941C325.342 207.237 323.325 199.107 327.022 192.942C330.767 186.695 338.822 184.751 344.964 188.456C351.186 192.208 353.164 200.209 349.42 206.455ZM342.312 202.601C340.914 204.93 337.906 205.615 335.587 204.21C333.348 202.857 332.538 199.878 333.936 197.544C335.334 195.215 338.342 194.53 340.581 195.883C342.901 197.283 343.715 200.267 342.312 202.596L342.312 202.601Z" fill="white"/>
      <path d="M90.335 313.058C132.387 277.235 169.767 250.758 213.377 239.855" stroke="#020000" stroke-width="12.46" stroke-miterlimit="10"/>
      <path d="M452.5 238.61C452.5 238.61 507 245.61 485.5 305.11" stroke="#020000" stroke-width="12.46" stroke-miterlimit="10"/>
      <ellipse cx="294.368" cy="523.32" rx="163.538" ry="21.805" fill="black" fill-opacity="0.1"/>
      <path d="M241.362 412.555L216.441 521.919" stroke="#020000" stroke-width="12.46" stroke-miterlimit="10"/>
      <path d="M366.441 405.145L364.954 514.51" stroke="#020000" stroke-width="12.46" stroke-miterlimit="10"/>
      <circle cx="377.649" cy="211.259" r="81" transform="rotate(-46.5019 377.649 211.259)" fill="#020000" stroke="#BFE5FB" stroke-width="20"/>
      <path d="M484.375 315.978L455.722 288.789" stroke="#BFE5FB" stroke-width="35" stroke-linecap="round"/>
      <path d="M457.173 290.166L439.763 273.646" stroke="#BFE5FB" stroke-width="17"/>
      <path fill-rule="evenodd" clip-rule="evenodd" d="M424.948 253.814C401.392 279.961 360.929 281.966 334.717 258.305C308.725 234.834 306.558 194.337 330.136 168.192C353.502 142.265 393.955 140.248 419.97 163.721C446.17 187.392 448.326 227.877 424.96 253.804L424.948 253.814ZM391.736 224.071C384.835 231.721 373.107 232.3 365.436 225.39C357.975 218.656 357.356 206.92 364.247 199.257C370.951 191.815 382.678 191.236 390.139 197.969C397.809 204.902 398.428 216.639 391.724 224.081L391.736 224.071Z" fill="white"/>
      <path fill-rule="evenodd" clip-rule="evenodd" d="M443.441 224.61C444.521 219.798 445.09 214.793 445.09 209.655C445.09 172.075 414.625 141.61 377.045 141.61C339.465 141.61 309 172.075 309 209.655C309 214.793 309.569 219.798 310.649 224.61C317.463 194.224 344.603 171.52 377.045 171.52C409.487 171.52 436.627 194.224 443.441 224.61Z" fill="black" fill-opacity="0.1"/>
      <path d="M485.5 305.11C494.361 280.587 490.313 264.982 482.49 255.11" stroke="#020000" stroke-width="12.46" stroke-miterlimit="10"/>
    </svg>
  </div>
</div>

## Objectives

- **Evaluate the performance of selected LLMs** in analyzing user feedback.
- **Assess the quality of responses** generated by LLMs to inquiries about corporate documents.
- **Identify strengths and weaknesses** of each model to guide future improvements.

## Results
Eleven respondents took part in the benchmark study, evaluating the outputs of the GPT-4, GPT Turbo, GPT-3.5, Claude 3 Opus, and Llama 3 models for the analysis of comments and answers to questions according to the corporate document through a blind review. On average, the models received the following points out of a possible 60 for each benchmark (**AVG PER RESP** indicates the average score for each model per benchmark, calculated as the total score divided by the 11 respondents who participated in the survey):

<table id="table1" class="display">
  <thead>
    <tr>
      <th>MODELS</th>
      <th>BENCH 1</th>
      <th>BENCH 2</th>
      <th>TOTAL</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>GPT4o/ GPT TURBO</td>
      <td>462</td>
      <td>326</td>
      <td>788</td>
    </tr>
    <tr>
      <td>GPT3.5</td>
      <td>463</td>
      <td>501</td>
      <td>964</td>
    </tr>
    <tr>
      <td>Claude 3 Opus</td>
      <td>412</td>
      <td>560</td>
      <td>972</td>
    </tr>
    <tr>
      <td>Llama 3</td>
      <td>408</td>
      <td>382</td>
      <td>790</td>
    </tr>
  </tbody>
</table>
<br>
<table id="table2" class="display">
  <thead>
    <tr>
      <th>MODELS</th>
      <th>AVG PER RESP</th>
      <th>AVG PER RESP BENCH 1</th>
      <th>AVG PER RESP BENCH 2</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>GPT4o/ GPT TURBO</td>
      <td>71.64</td>
      <td>42</td>
      <td>29.64</td>
    </tr>
    <tr>
      <td>GPT3.5</td>
      <td>87.64</td>
      <td>42.09</td>
      <td>45.55</td>
    </tr>
    <tr>
      <td>Claude 3 Opus</td>
      <td>88.36</td>
      <td>37.45</td>
      <td>50.91</td>
    </tr>
    <tr>
      <td>Llama 3</td>
      <td>71.82</td>
      <td>37.09</td>
      <td>34.73</td>
    </tr>
  </tbody>
</table>

<p>The table below shows the cost per 1000 tokens for each model, providing a clear comparison of the price for processing tokens across different models. This parameter is separate and does not accumulate into the overall score.</p>

<table id="table3" class="display">
  <thead>
    <tr>
      <th>MODELS</th>
      <th>PRICE PER 1000 TOKENS ($)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>GPT4o/ GPT TURBO</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>GPT3.5</td>
      <td>0.001</td>
    </tr>
    <tr>
      <td>Claude 3 Opus</td>
      <td>0.045</td>
    </tr>
    <tr>
      <td>Llama 3</td>
      <td>0.00586</td>
    </tr>
  </tbody>
</table>

<p>We tracked the activity of 11 respondents in the evaluation and obtained averaged data on the number of points distributed by them in the context of filling out the questionnaire. In the future, this will allow us to validate the conscientiousness of filling out the questionnaire and will help prevent the results from being distorted by careless responses, ensuring the accuracy of the benchmark results.</p>

<table id="table4" class="display">
  <thead>
    <tr>
      <th>RESPONDENT ID</th>
      <th>TOTAL BENCH 1</th>
      <th>TOTAL BENCH 2</th>
      <th>TOTAL SUM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>173</td>
      <td>140</td>
      <td>313</td>
    </tr>
    <tr>
      <td>2</td>
      <td>160</td>
      <td>168</td>
      <td>328</td>
    </tr>
    <tr>
      <td>3</td>
      <td>171</td>
      <td>176</td>
      <td>347</td>
    </tr>
    <tr>
      <td>4</td>
      <td>176</td>
      <td>154</td>
      <td>330</td>
    </tr>
    <tr>
      <td>5</td>
      <td>129</td>
      <td>156</td>
      <td>285</td>
    </tr>
    <tr>
      <td>6</td>
      <td>187</td>
      <td>179</td>
      <td>366</td>
    </tr>
    <tr>
      <td>7</td>
      <td>154</td>
      <td>156</td>
      <td>310</td>
    </tr>
    <tr>
      <td>8</td>
      <td>128</td>
      <td>196</td>
      <td>324</td>
    </tr>
    <tr>
      <td>9</td>
      <td>207</td>
      <td>195</td>
      <td>402</td>
    </tr>
    <tr>
      <td>10</td>
      <td>120</td>
      <td>115</td>
      <td>235</td>
    </tr>
    <tr>
      <td>11</td>
      <td>140</td>
      <td>134</td>
      <td>274</td>
    </tr>
  </tbody>
  <tfoot>
    <tr>
      <td><b>AVG TOTAL</b></td>
      <td>159</td>
      <td>161</td>
      <td>319</td>
    </tr>
  </tfoot>
</table>

<style>
  .styled-table table {
    width: 100%;
    border-collapse: collapse;
    margin-left: auto;
    margin-right: auto;
  }

  .styled-table th, .styled-table td {
    border: 1px solid #ddd;
    padding: 8px;
    text-align: left;
  }

  .styled-table th {
    background-color: #f2f2f2;
  }

  .content-with-image {
    display: flex;
    align-items: flex-start;
  }

  .text-content {
    flex: 1;
  }

  .image-content {
    flex: 1;
    display: flex;
    justify-content: center;
  }

  div.dataTables_filter {
    margin-bottom: 3px;
  }

   /* Selected column color */
  table.dataTable tbody tr.selected td,
  table.dataTable tbody th.selected,
  table.dataTable tbody td.selected {
    background-color: #efefef !important; /* Change this color to your preference */
  }

  /* Apply the style to all selected cells */
  table.dataTable tbody td.sorting_1,
  table.dataTable tbody td.sorting_2,
  table.dataTable tbody td.sorting_3 {
    background-color: #efefef !important; /* Change this color to your preference */
  }

  /* Apply the style to header as well */
  table.dataTable thead th.sorting_1,
  table.dataTable thead th.sorting_2,
  table.dataTable thead th.sorting_3 {
    background-color: #efefef !important; /* Change this color to your preference */
  }
</style>

<script>
  $(document).ready(function() {
    $('#table1').DataTable({
      "pageLength": 25,
      "footerCallback": function(row, data, start, end, display) {
      }
    });
    $('#table2').DataTable({
      "pageLength": 25,
      "footerCallback": function(row, data, start, end, display) {
      }
    });
    $('#table3').DataTable({
      "pageLength": 25,
      "footerCallback": function(row, data, start, end, display) {
      }
    });
    $('#table4').DataTable({
      "pageLength": 25,
      "footerCallback": function(row, data, start, end, display) {
      }
    });
  });
</script>

<div>
* It should be noted separately that for bencmark #1 we used finetuned llama-3 and gpt-3.5, while for bencmark #2 we used general ones.
<br>
** Price is avarage between input and ouput tokens.
</div>

## Assessment

Participants were expected to be familiar with the input data, the results from the LLMs, and the evaluation criteria outlined in our methodology. This enabled effective participation in the study and helped achieve our goal of rating LLM models.

### Benchmark #1

The questionnaire included 20 questions, each of which evaluated compliance with the analysis criterion in the outputs of 4 models. The results of the analysis of 4 feedbacks were taken into the study.

Participants found an example of feedback along with the analysis results from four selected language models. The feedback analysis included:

- The categories used for evaluation,
- Alerts that appeared in cases of extremely critical feedback or exceptionally positive service,
- Suggestions for service improvement based on the evaluation,
- The automatically generated response to the feedback content,
- A summary providing concise information about the main content of the comment and its level of criticality.

The feedbacks were sourced from the **NHS Choices reviews from the beginning up to 17 Dec 2017**. This dataset is publicly available in the [Harvard Dataverse](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/AXZDKC).

#### Methodology for evaluating feedback analysis

A scoring system ranging from 0 to 3 was used to evaluate the quality of the feedback analysis based on five parameters:

1. **Categories**
   - **0**: Categories were missing or irrelevant.
   - **1**: Categories were somewhat accurate but lacked detail or rationale.
   - **2**: Categories were mostly accurate and informative but might have missed minor points.
   - **3**: Categories were detailed, accurate, and provided a comprehensive rationale.

2. **Alert Reason**
   - If no alert was provided, the reasonability of its absence was evaluated from 0 to 3 (0 - alert definitely should have been provided; 3 - alert absence was fully reasonable).
   - If an alert was provided:
     - **0**: Alert reason was irrelevant.
     - **1**: Alert reason was somewhat related but lacked clarity or completeness.
     - **2**: Alert reason was mostly accurate and concise but could have been refined.
     - **3**: Alert reason was precise, accurate, and succinctly captured the main issue.

3. **Suggestions**
   - **0**: Suggestions were missing for negative comments (they might not have been provided for positive comments) or irrelevant.
   - **1**: Suggestions were partially relevant but lacked detail or completeness.
   - **2**: Suggestions addressed most issues but might have missed minor points or lacked full practicality.
   - **3**: Suggestions were thorough, relevant, and actionable, covering all key points.

4. **Auto-response**
   - **0**: Auto-response was missing or inappropriate.
   - **1**: Auto-response acknowledged the issue but lacked friendliness, formality, or clarity.
   - **2**: Auto-response was mostly friendly, formal, and clear but could have been improved.
   - **3**: Auto-response was well-crafted, balancing friendliness and formality, and clearly addressed the user's opinion.

5. **Summary**
   - **0**: Summary was missing or irrelevant.
   - **1**: Summary was somewhat clear but lacked completeness or conciseness.
   - **2**: Summary was mostly clear and concise but might have missed minor details.
   - **3**: Summary was comprehensive, clear, concise, and included all required elements.

#### Maximum Score for Benchmark #1

Each question had a maximum score of 3, and there were 20 questions. Therefore, the maximum score for each model in Benchmark #1 was:

**20 questions * 3 points = 60 points.**

### Benchmark #2

For the second benchmark, participants found the answers of four selected language models about GreenM company and staff management (regulations, procedures, benefits). The informational resource for answers evaluation was the GreenM Handbook.

#### Methodology for Evaluating Answers

For the five answers, a scoring system ranging from 0 to 3 was used to evaluate the quality of the answer based on four parameters:

1. **Informativeness**
   - **0**: The answer did not provide useful information.
   - **1**: The answer provided less useful information than useful information.
   - **2**: The answer provided more useful information than less useful.
   - **3**: The answer provided useful information.

2. **Correctness**
   - **0**: The answer contained factual errors or incorrect information.
   - **1**: The answer seemed to have significantly more errors than truth.
   - **2**: The answer seemed to be correct, but some details were missing.
   - **3**: The answer was factually correct and contained no errors.

3. **Relevance**
   - **0**: The model's response did not address the question.
   - **1**: The model's answer was rather irrelevant than relevant.
   - **2**: The model's answer was rather relevant than irrelevant.
   - **3**: The model's answer was exactly relevant to the question.

4. **Emotional Responsiveness**
   - **0**: The model's response seemed ruder or contained more inappropriate reactions than expected.
   - **1**: The model ignored or misinterpreted the user's emotional state, possibly reacting inappropriately or rudely.
   - **2**: The model responded with less empathy or kindness than expected but in a polite way.
   - **3**: The model responded appropriately to the user's emotional state by providing empathy or adequate emotional feedback.

#### Maximum Score for Benchmark #2

Each question had a maximum score of 3 for 1 parameter, there were 4 parameters, and there were 5 questions. Therefore, the maximum score for each model in Benchmark #2 was:

**4 parameters * 3 points * 5 questions = 60 points.**

### Total Maximum Score

The total maximum score for each model, combining both benchmarks, was:

**60 points (Benchmark #1) + 60 points (Benchmark #2) = 120 points.**